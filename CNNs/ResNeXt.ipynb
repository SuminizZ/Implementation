{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"toc_visible":true,"authorship_tag":"ABX9TyNLG7yfOlRD+HZxprb22TWi"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"neAJz7B6R1T1","executionInfo":{"status":"ok","timestamp":1686114920739,"user_tz":-540,"elapsed":6603,"user":{"displayName":"Noah","userId":"01705723788756664405"}},"outputId":"66fcbc88-e8aa-459c-e15b-536ec2464882"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: torchinfo in /usr/local/lib/python3.10/dist-packages (1.8.0)\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","import sys\n","sys.path.append(\"/content/drive/MyDrive/Legend13\")\n","import torch \n","import torch.nn as nn\n","!pip install torchinfo\n","from torchinfo import summary"]},{"cell_type":"code","source":["!jupyter nbconvert --to markdown /content/drive/MyDrive/Legend13/Self_Implement/psm_ResNeXt.ipynb"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1yTASxOnS448","executionInfo":{"status":"ok","timestamp":1686119035688,"user_tz":-540,"elapsed":3119,"user":{"displayName":"Noah","userId":"01705723788756664405"}},"outputId":"d1cf2883-c05d-4abd-b1ee-8397f89dbd99"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[NbConvertApp] Converting notebook /content/drive/MyDrive/Legend13/Self_Implement/psm_ResNeXt.ipynb to markdown\n","[NbConvertApp] Writing 20060 bytes to /content/drive/MyDrive/Legend13/Self_Implement/psm_ResNeXt.md\n"]}]},{"cell_type":"markdown","source":["## **Split-Transform-Merge Strategy**\n","\n","<br/>\n","\n","- Grouped convolution performs a set of transformations that consist of a low-dimensional embedding and aggregation of output. \n","- All transformations are in same topology, which distinguishes ResNeXt from Inception-ResNet (Inception v4) that also involves branching and concatenating in residual function.\n","\n","&emsp;&emsp;&emsp;**Figure 1. : Original bottleneck block in ResNet vs ResNeXt with caridnality = 32 with roughly same complexity**\n","\n","&emsp;&emsp;&emsp;<img src=\"https://github.com/SuminizZ/Physics/assets/92680829/50bc4d51-21c5-4844-8a37-2ff86536e0f0\" width=\"600\">\n","\n","<br/>\n","\n","### **Aggregated Transformation**\n","\n","<br/>\n","\n","- $\\large y = x + \\sum_{i=1}^{C} T_i(x)$, &emsp; where $\\large T_i(x)$ has same topology for all $\\large i$\n","\n","<br/>\n","\n","## **Equivalent Building Blocks for ResNeXt**\n","\n","<br/>\n","\n","&emsp;&emsp;&emsp;**Figure 3. : Equivalent blcoks for ResNeXt (<span style=\"color:blue\">Figure 3-(c)</span> is the final implementation of gouped convolution)**\n","\n","&emsp;&emsp;&emsp;<img src=\"https://github.com/SuminizZ/Physics/assets/92680829/6ec35f31-6ee4-4311-9280-74cad1ccd068\" width=\"900\">\n","\n","\n","<br/>\n","\n","## **Basic Building Block (Bottleneck)** \n","\n","<br/>\n","\n","- Example of basic bottleneck building block in Conv2 stage of ResNeXt architecture\n","\n","&emsp;&emsp;&emsp;<img src=\"https://github.com/SuminizZ/Physics/assets/92680829/7f9d4a29-aafc-4ff4-977e-8f976e6b0ba0\" width=\"200\">\n","\n","&emsp;&emsp;&emsp;<img src=\"https://github.com/SuminizZ/Physics/assets/92680829/36388eba-896e-4904-9cf0-2b7451c263f1\" width=\"600\">\n","\n","<br/>\n","\n","- grouped convolution layer is a **wider and sparsely-connected** version of original ResNeXt in Figure 1. (left)\n","\n","<br/>\n","\n","## **Comparison with Other Neural Architectures**\n","\n","<br/>\n","\n","### **1. Cardinality vs Width**\n","\n","<br/>\n","\n","&emsp;&emsp;&emsp;**Table 2. : Trade-Off between cardinality and width of block (d) with preserved complexity**\n","\n","&emsp;&emsp;&emsp;<img src=\"https://github.com/SuminizZ/Physics/assets/92680829/59de951b-1f17-479e-b8ae-442725d8c07a\" width=\"500\">\n","\n","- Example of how to determine appropriate trade-off combination of cardinality and bottleneck width while maintaining the complexity \n","\n","    - 1) Original ResNet bottleneck block has 256 x 64 + 3 x 3 x 64 x 64 + 64 x 256 ≈ 70k parameters\n","    - 2) With bottleneck width = d, parameters = C · (256 · d + 3 · 3 · d · d + d · 256) \n","    - find the combination of C and d that gives roughly identical complexity.\n","\n","<br/>\n","\n","&emsp;&emsp;&emsp;**Figure 5. Comparison of train and validation accuracy between ResNet 50, 101 (1 x 64d) and ResNeXt-50, 101 (32 x 4d)**\n","\n","&emsp;&emsp;&emsp;<img src=\"https://github.com/SuminizZ/Physics/assets/92680829/d58a61a3-9812-41c0-bdeb-d958cc243568\" width=\"900\">\n","\n","&emsp;&emsp;&emsp;**Table 3. : Results of increasing cardinality at the expense of bottleneck width (d) (Experiments on ImageNet-1K)**\n","\n","&emsp;&emsp;&emsp;<img src=\"https://github.com/SuminizZ/Physics/assets/92680829/4b75d1e6-a4ca-4a95-a022-d173fda95793\" width=\"450\">\n","\n","<br/>\n","\n","- for both 50 and 101 layer networks, every ResNeXt with cardinality increasing from 2 to 32 outperforms its ResNet couterpart. \n","\n","- Because the improvement from trade-off becomes saturated as cardinality increases, the paper adopted the bottleneck width no smaller than 4d. \n","\n","\n","<br/>\n","\n","### **2. Cardinality vs Depth/Width with Increased Complexity**\n","\n","<br/>\n","\n","&emsp;&emsp;&emsp;**Table 4. : Comparison of deeper/wider/increased cardinality networks with complexity doubled to ResNet-101's**\n","\n","&emsp;&emsp;&emsp;<img src=\"https://github.com/SuminizZ/Physics/assets/92680829/a84ca011-8686-4dd7-9272-1703262cc028\" width=\"500\">\n","\n","<br/>\n","\n","- highlighted part indicates where the complexity is increased (baseline : 1x complexity reference)\n","\n","- ResNeXt-101 models with increased cardinality (2 x 64d, 64 x 4d) record lower top-1 and top-5 error rate compared to both deeper and wider ResNet.\n","\n","- What's remarkable here is that ResNeXt with 1x complexity (32 x 4d) performs better than ResNet-200 and wider ResNet-101 even with 50% complexity. \n","\n","<br/>\n","\n","&emsp;&emsp;&emsp;**Figure 7. Effects of Increasing Complexity (Parameters) with Cardinality vs Width (on CIFAR-10)**\n","\n","&emsp;&emsp;&emsp;<img src=\"https://github.com/SuminizZ/Physics/assets/92680829/1397d0af-80c7-4a59-97c9-3289a6416885\" width=\"500\">\n","\n","<br/>\n","\n","- It's always better to increase cardinality instead of transformation width to reduce the test error rate \n","\n","<br/>\n","\n","### **3. ResNet vs Inception Net vs ResNeXt**\n","\n","<br/>\n","\n","&emsp;&emsp;&emsp;**Table 5. experimented on ImageNet-1K**\n","\n","&emsp;&emsp;&emsp;<img src=\"https://github.com/SuminizZ/Physics/assets/92680829/8ff68e2f-a1cd-48e9-84e9-22e7ea28e60e\" width=\"530\">\n","\n","\n","<br/>\n","\n","### **4. With or Without Residual (Skip) Connections**\n","\n","<br/>\n","\n","&emsp;&emsp;&emsp;<img src=\"https://github.com/SuminizZ/Physics/assets/92680829/6ee04d46-eaff-4165-ad84-e78cb93e53fe\" width=\"420\">\n","\n","\n","- Introducing grouped convolution method shows consistent improvement in the performance of networks with or without skip connections.\n","\n","- Residual connections are known to relieve non-convexities existing in the loss surface, resulting in easier and faster optimization. \n","\n","- Considering all these, residual connections are helpful\n","for optimization process, whereas aggregated transformations achieve\n","stronger representations where networks learn to capture more representationally important patterns or features of the input images.  "],"metadata":{"id":"TFrNOacAS6SN"}},{"cell_type":"markdown","source":["## **ResNeXt Architecture**\n","\n","<br/>\n","\n","&emsp;&emsp;&emsp;<img src=\"https://github.com/SuminizZ/Physics/assets/92680829/a03bfa29-4755-4461-9bf9-3f928365b7f8\" width=\"600\">\n","\n","<br/>\n","\n","- Performs downsampling with stride 2 3x3 grouped convolution for every first block at each conv stage. \n","\n"],"metadata":{"id":"g2vMYfqsS7mE"}},{"cell_type":"markdown","source":["## **Implementation with PyTorch**"],"metadata":{"id":"rFJDL-cKmgQ-"}},{"cell_type":"code","source":["class Bottleneck(nn.Module):\n","    expansion = 2\n","    def __init__(self, in_channels, out_channels, cardinality=32, stride=1, projection=False):\n","        super().__init__()\n","\n","        self.residual = nn.Sequential(nn.Conv2d(in_channels, out_channels, 1, bias=False),\n","                                    nn.BatchNorm2d(out_channels),\n","                                    nn.ReLU(inplace=True),\n","                                    nn.Conv2d(out_channels, out_channels, 3, stride=stride, padding=1, groups=cardinality, bias=False),\n","                                    nn.BatchNorm2d(out_channels),\n","                                    nn.ReLU(inplace=True),\n","                                    nn.Conv2d(out_channels, out_channels*self.expansion, 1, bias=False),\n","                                    nn.BatchNorm2d(out_channels*self.expansion))\n","        if projection:\n","            self.shortcut = nn.Sequential(nn.Conv2d(in_channels, out_channels*self.expansion, 1, stride=stride),\n","                                          nn.BatchNorm2d(out_channels*self.expansion))\n","        else:\n","            self.shortcut = nn.Identity()\n","\n","        self.relu = nn.ReLU()\n","\n","    def forward(self, x):\n","        residual = self.residual(x)\n","        shortcut = self.shortcut(x)\n","\n","        return self.relu(residual + shortcut)\n","\n","\n","class ResNeXt(nn.Module):\n","    def __init__(self, in_channels, block, expansion, cardinality, block_repeats, num_classes, zero_init_residual=True):\n","        super().__init__()\n","\n","        self.conv1 = nn.Sequential(nn.Conv2d(in_channels, 64, 7, stride=2, padding=3),\n","                                   nn.BatchNorm2d(64),\n","                                   nn.ReLU(inplace=True))\n","        self.conv2_pool = nn.MaxPool2d(3, stride=2, padding=1)\n","\n","        self.expansion = expansion\n","        out_channels, self.conv2_blocks = self.stack_blocks(block, 64, block_repeats[0], cardinality, 2)\n","        out_channels, self.conv3_blocks = self.stack_blocks(block, out_channels, block_repeats[1], cardinality, 2)\n","        out_channels, self.conv4_blocks = self.stack_blocks(block, out_channels, block_repeats[2], cardinality, 2)\n","        out_channels, self.conv5_blocks = self.stack_blocks(block, out_channels, block_repeats[3], cardinality, 2)\n","\n","        self.gap = nn.AdaptiveAvgPool2d((1,1))   # 1x1x2048\n","        self.classifier = nn.Linear(out_channels, 1000)\n","\n","        for m in self.modules():\n","            if isinstance(m, nn.Conv2d):\n","                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n","            elif isinstance(m, nn.BatchNorm2d):\n","                nn.init.constant_(m.weight, 1)\n","                nn.init.constant_(m.bias, 0)\n","            # zero-initialize the last BN in each residual branch -> set the start of residual branch as zero (identity mapping)\n","            # improves model by 0.2~0.3%p (https://arxiv.org/abs/1706.02677)\n","            elif isinstance(m, block):    \n","                nn.init.constant_(m.residual[-1].weight, 0)\n","\n","    def stack_blocks(self, block, in_channel, block_repeat, cardinality, stride):\n","        stacked = []\n","\n","        c, repeats = block_repeat\n","        for _ in range(repeats):\n","            if stride == 2 or in_channel != c*self.expansion:\n","                stacked += [block(in_channel, c, cardinality, stride, True)]\n","                in_channel = c*self.expansion\n","                stride = 1\n","            else:\n","                stacked += [block(in_channel, c, cardinality)]\n","                in_channel = c*self.expansion\n","\n","        return c*self.expansion, nn.Sequential(*stacked)  \n","\n","    def forward(self, x):\n","        x = self.conv1(x)\n","        x = self.conv2_pool(x)\n","        x = self.conv2_blocks(x)\n","        x = self.conv3_blocks(x)\n","        x = self.conv4_blocks(x)\n","        x = self.conv5_blocks(x)\n","        x = self.gap(x)\n","        x = torch.flatten(x, start_dim=1)\n","        out = self.classifier(x)\n","\n","        return out"],"metadata":{"id":"LXBNq1XRS7tU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## **Model Summary**\n"],"metadata":{"id":"UjSh5rGzS7pz"}},{"cell_type":"code","source":["block_repeats = [(128,3), (256,4), (512,6), (1024,3)]\n","expansion = 2\n","cardinality = 32\n","\n","model = ResNeXt(3, Bottleneck, expansion, cardinality, block_repeats, 1000)\n","summary(model, input_size=(2, 3, 224, 224), device='cpu')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0OnqFs7OS7ws","executionInfo":{"status":"ok","timestamp":1686114954540,"user_tz":-540,"elapsed":967,"user":{"displayName":"Noah","userId":"01705723788756664405"}},"outputId":"c67841a6-6c03-439e-fc3f-462fefe783b8"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["==========================================================================================\n","Layer (type:depth-idx)                   Output Shape              Param #\n","==========================================================================================\n","ResNeXt                                  [2, 1000]                 --\n","├─Sequential: 1-1                        [2, 64, 112, 112]         --\n","│    └─Conv2d: 2-1                       [2, 64, 112, 112]         9,472\n","│    └─BatchNorm2d: 2-2                  [2, 64, 112, 112]         128\n","│    └─ReLU: 2-3                         [2, 64, 112, 112]         --\n","├─MaxPool2d: 1-2                         [2, 64, 56, 56]           --\n","├─Sequential: 1-3                        [2, 256, 28, 28]          --\n","│    └─Bottleneck: 2-4                   [2, 256, 28, 28]          --\n","│    │    └─Sequential: 3-1              [2, 256, 28, 28]          46,592\n","│    │    └─Sequential: 3-2              [2, 256, 28, 28]          17,152\n","│    │    └─ReLU: 3-3                    [2, 256, 28, 28]          --\n","│    └─Bottleneck: 2-5                   [2, 256, 28, 28]          --\n","│    │    └─Sequential: 3-4              [2, 256, 28, 28]          71,168\n","│    │    └─Identity: 3-5                [2, 256, 28, 28]          --\n","│    │    └─ReLU: 3-6                    [2, 256, 28, 28]          --\n","│    └─Bottleneck: 2-6                   [2, 256, 28, 28]          --\n","│    │    └─Sequential: 3-7              [2, 256, 28, 28]          71,168\n","│    │    └─Identity: 3-8                [2, 256, 28, 28]          --\n","│    │    └─ReLU: 3-9                    [2, 256, 28, 28]          --\n","├─Sequential: 1-4                        [2, 512, 14, 14]          --\n","│    └─Bottleneck: 2-7                   [2, 512, 14, 14]          --\n","│    │    └─Sequential: 3-10             [2, 512, 14, 14]          217,088\n","│    │    └─Sequential: 3-11             [2, 512, 14, 14]          132,608\n","│    │    └─ReLU: 3-12                   [2, 512, 14, 14]          --\n","│    └─Bottleneck: 2-8                   [2, 512, 14, 14]          --\n","│    │    └─Sequential: 3-13             [2, 512, 14, 14]          282,624\n","│    │    └─Identity: 3-14               [2, 512, 14, 14]          --\n","│    │    └─ReLU: 3-15                   [2, 512, 14, 14]          --\n","│    └─Bottleneck: 2-9                   [2, 512, 14, 14]          --\n","│    │    └─Sequential: 3-16             [2, 512, 14, 14]          282,624\n","│    │    └─Identity: 3-17               [2, 512, 14, 14]          --\n","│    │    └─ReLU: 3-18                   [2, 512, 14, 14]          --\n","│    └─Bottleneck: 2-10                  [2, 512, 14, 14]          --\n","│    │    └─Sequential: 3-19             [2, 512, 14, 14]          282,624\n","│    │    └─Identity: 3-20               [2, 512, 14, 14]          --\n","│    │    └─ReLU: 3-21                   [2, 512, 14, 14]          --\n","├─Sequential: 1-5                        [2, 1024, 7, 7]           --\n","│    └─Bottleneck: 2-11                  [2, 1024, 7, 7]           --\n","│    │    └─Sequential: 3-22             [2, 1024, 7, 7]           864,256\n","│    │    └─Sequential: 3-23             [2, 1024, 7, 7]           527,360\n","│    │    └─ReLU: 3-24                   [2, 1024, 7, 7]           --\n","│    └─Bottleneck: 2-12                  [2, 1024, 7, 7]           --\n","│    │    └─Sequential: 3-25             [2, 1024, 7, 7]           1,126,400\n","│    │    └─Identity: 3-26               [2, 1024, 7, 7]           --\n","│    │    └─ReLU: 3-27                   [2, 1024, 7, 7]           --\n","│    └─Bottleneck: 2-13                  [2, 1024, 7, 7]           --\n","│    │    └─Sequential: 3-28             [2, 1024, 7, 7]           1,126,400\n","│    │    └─Identity: 3-29               [2, 1024, 7, 7]           --\n","│    │    └─ReLU: 3-30                   [2, 1024, 7, 7]           --\n","│    └─Bottleneck: 2-14                  [2, 1024, 7, 7]           --\n","│    │    └─Sequential: 3-31             [2, 1024, 7, 7]           1,126,400\n","│    │    └─Identity: 3-32               [2, 1024, 7, 7]           --\n","│    │    └─ReLU: 3-33                   [2, 1024, 7, 7]           --\n","│    └─Bottleneck: 2-15                  [2, 1024, 7, 7]           --\n","│    │    └─Sequential: 3-34             [2, 1024, 7, 7]           1,126,400\n","│    │    └─Identity: 3-35               [2, 1024, 7, 7]           --\n","│    │    └─ReLU: 3-36                   [2, 1024, 7, 7]           --\n","│    └─Bottleneck: 2-16                  [2, 1024, 7, 7]           --\n","│    │    └─Sequential: 3-37             [2, 1024, 7, 7]           1,126,400\n","│    │    └─Identity: 3-38               [2, 1024, 7, 7]           --\n","│    │    └─ReLU: 3-39                   [2, 1024, 7, 7]           --\n","├─Sequential: 1-6                        [2, 2048, 4, 4]           --\n","│    └─Bottleneck: 2-17                  [2, 2048, 4, 4]           --\n","│    │    └─Sequential: 3-40             [2, 2048, 4, 4]           3,448,832\n","│    │    └─Sequential: 3-41             [2, 2048, 4, 4]           2,103,296\n","│    │    └─ReLU: 3-42                   [2, 2048, 4, 4]           --\n","│    └─Bottleneck: 2-18                  [2, 2048, 4, 4]           --\n","│    │    └─Sequential: 3-43             [2, 2048, 4, 4]           4,497,408\n","│    │    └─Identity: 3-44               [2, 2048, 4, 4]           --\n","│    │    └─ReLU: 3-45                   [2, 2048, 4, 4]           --\n","│    └─Bottleneck: 2-19                  [2, 2048, 4, 4]           --\n","│    │    └─Sequential: 3-46             [2, 2048, 4, 4]           4,497,408\n","│    │    └─Identity: 3-47               [2, 2048, 4, 4]           --\n","│    │    └─ReLU: 3-48                   [2, 2048, 4, 4]           --\n","├─AdaptiveAvgPool2d: 1-7                 [2, 2048, 1, 1]           --\n","├─Linear: 1-8                            [2, 1000]                 2,049,000\n","==========================================================================================\n","Total params: 25,032,808\n","Trainable params: 25,032,808\n","Non-trainable params: 0\n","Total mult-adds (G): 2.44\n","==========================================================================================\n","Input size (MB): 1.20\n","Forward/backward pass size (MB): 145.72\n","Params size (MB): 100.13\n","Estimated Total Size (MB): 247.05\n","=========================================================================================="]},"metadata":{},"execution_count":4}]},{"cell_type":"code","source":["x = torch.randn(2,3,224,224)\n","out = model(x)\n","print(out.shape)\n","out"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_IJTLRa0S70E","executionInfo":{"status":"ok","timestamp":1686115263149,"user_tz":-540,"elapsed":436,"user":{"displayName":"Noah","userId":"01705723788756664405"}},"outputId":"aaf4ba6e-2fda-4485-af99-97def897f0d3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([2, 1000])\n"]},{"output_type":"execute_result","data":{"text/plain":["tensor([[ 1.3636,  1.0331, -0.1438,  ...,  0.6048,  0.3152,  0.3505],\n","        [ 1.3330,  0.8973, -0.1342,  ...,  0.5798,  0.0394,  0.3434]],\n","       grad_fn=<AddmmBackward0>)"]},"metadata":{},"execution_count":5}]},{"cell_type":"markdown","source":["## **Other types of Convolution**\n","\n","<br/>\n","\n","- In computer vision, it is important to retain enough contextual information to make accurate classification for a given input image. This can be done by increasing the kernel size of receptive field or concatenating multiple convolution layers (5x5 = 3x3 + 3x3). These approaches, However, dramatically increase the computational complexities. To tackle this issue, researchers have developed a variety of other convolution methods that aim to reduce the computations while preserving the spatial information. Here are a few examples of these convolution methods.\n","\n","<br/>\n","\n","\n","### **Depthwise Pointwise Convolution**\n","\n","<br/>\n","\n","- Traditional convolution layers perform spatial and channel-wise convolution at the same time, which means a single output feature requries a separate set of filters with complete shape. \n","\n","- Depthwise pointwise convolution, on the other hand, splits the convolution operation into two separate stages: depthwise convolution and pointwise convolution. \n","\n","&emsp;&emsp;&emsp; Depthwise Convolution & Pointwise Convolution\n","\n","&emsp;&emsp;&emsp;<img src=\"https://github.com/SuminizZ/Physics/assets/92680829/15328448-2431-4f00-80ab-013f316a2ba3\" width=\"400\">\n","\n","&emsp;&emsp;&emsp;<img src=\"https://github.com/SuminizZ/Physics/assets/92680829/e33db26f-34eb-4dd3-a5f8-8f6e7dd11497\" width=\"400\"> \n","\n","\n","1. **Depthwise Convolution** : \n","    - Idential to the grouped convolution with cardinality equal to total channel size. \n","    - Only performs spatial convolution where each channel of the input requires a single filter and concatenates the resultant feature maps.\n","    - Each feature map contains spatial representation within one chanenl. \n","    - This reduces the network complexities, as each filter is only responsible for convolving with its corresponding input channel.   \n","\n","2. **Pointwise Convolution** : \n","    - Applying convolution with size 1, stride 1, merging the output of depthwise convolution into a desired number of feature maps. \n","    - Creates new representations by linear combination of all channels with learned weights. \n","\n","- One can improve model capacity by increasing parameters using the saved resources from depthwise convolution\n","\n","<br/>\n","\n","### **Depthwise Separable Convolution**\n","\n","<br/>\n","\n","&emsp;&emsp;&emsp;<img src=\"https://github.com/SuminizZ/Physics/assets/92680829/0fe43c30-9765-47c9-8198-68d96241ea23\" width=\"600\"> \n","\n","&emsp;&emsp;&emsp;<img src=\"https://github.com/SuminizZ/Physics/assets/92680829/421bbbfb-1e45-42f0-8993-b4ac0833b506\" width=\"420\"> \n","\n","<br/>\n","\n","- Same as the former one, but last combining step with pointwise convolution is replaced with separable convoluiton where nxn convolution is splited into 1xn convolution (horizontal) and nx1 convolution (vertical)\n","\n"],"metadata":{"id":"TKis06DMS6WX"}},{"cell_type":"code","source":[],"metadata":{"id":"LFlEg_fomemO"},"execution_count":null,"outputs":[]}]}